{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"final-multi-label-text-classification-with-goemotions-dataset.ipynb","provenance":[],"collapsed_sections":["0UIWH9eLJJsm","lV2aiiqSMhgs"],"machine_shape":"hm","authorship_tag":"ABX9TyM1J48Aqq7rWnPZrsoOxpdA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Lqccfp2d9fQz"},"source":["# Multi-label Text Classification with BERT and PyTorch Lightning, finetuned on GoEmotions [Script B]"]},{"cell_type":"markdown","metadata":{"id":"_2xfDbC7AaQA"},"source":["*This notebook contains the script used to build our first multi-label classification model, which recognizes emotions from reviews collected in Script A. In it, we build a BERTBase model using the PyTorch Lightning library. Note that this notebook's code was written following a tutorial on multi-label text classification for detection of toxic tweets published by Venelin Valkov. However, the content of the code was written to serve our own model goals.*\n","\n","---\n","*References: https://curiousily.com/posts/multi-label-text-classification-with-bert-and-pytorch-lightning/*\n"]},{"cell_type":"markdown","metadata":{"id":"aHoWa7koCrq9"},"source":["## Package Installation, Imports & Setup"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"04qYrpNG9GSE","executionInfo":{"status":"ok","timestamp":1630580222395,"user_tz":-60,"elapsed":194,"user":{"displayName":"Mberry Seck","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjE-j61hYX6QQG1ZVNUmkK2rCI-N96aQQfFuiX7=s64","userId":"15407803474852230938"}},"outputId":"923e746e-4e31-4aa2-ec6b-1281fa88fa0a"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Thu Sep  2 10:57:02 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.57.02    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   35C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","metadata":{"id":"zGynKZWKClRZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630580223431,"user_tz":-60,"elapsed":469,"user":{"displayName":"Mberry Seck","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjE-j61hYX6QQG1ZVNUmkK2rCI-N96aQQfFuiX7=s64","userId":"15407803474852230938"}},"outputId":"4334a664-be91-4570-bdbb-a33194df79a9"},"source":["import torch\n","\n","# Check if a GPU is available. If it is, use it, otherwise, use CPU\n","if torch.cuda.is_available():      \n","    device = torch.device(\"cuda\")\n","    print('There are %d free GPUs.' % torch.cuda.device_count())\n","    print('GPU to use:', torch.cuda.get_device_name(0))\n","\n","else:\n","    print('GPUs are not available. CPU will be used')\n","    device = torch.device(\"cpu\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["There are 1 free GPUs.\n","GPU to use: Tesla T4\n"]}]},{"cell_type":"code","metadata":{"id":"l-VFsObeD6yb"},"source":["!pip install pytorch-lightning==1.2.8 --quiet\n","!pip install transformers==4.7.0 --quiet"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wOlAcODXEEZ5"},"source":["! pip install transformers datasets --quiet"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5x8x5P48EHt4"},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","import re\n","\n","import io\n","from google.colab import files\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from tqdm.auto import tqdm\n","\n","from transformers import BertTokenizerFast as BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n","\n","import pytorch_lightning as pl\n","from pytorch_lightning.metrics.functional import accuracy, f1, auroc\n","from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n","from pytorch_lightning.loggers import TensorBoardLogger\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, multilabel_confusion_matrix\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d3fKfqjkEn-1"},"source":["## Data Import & Cleaning"]},{"cell_type":"code","metadata":{"id":"-X_KTZpfEqDg"},"source":["#Import raw GoEmotions dataset from the HuggingFace dataset library\n","#and store in raw_df dataframe\n","from datasets import load_dataset\n","raw_dataset = load_dataset(\"go_emotions\", \"raw\")\n","raw_df = pd.DataFrame(raw_dataset['train'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BXbswGGhEwa5"},"source":["#Clean dataframe and keep relevant columns\n","EMOTION_LABELS = ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral']\n","keep_cols = ['text']+EMOTION_LABELS\n","raw_df = raw_df[keep_cols]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5o6Z6SscHU2a"},"source":["#Clean all text\n","\n","#Replace non-alphabetical characters with whitespaces\n","raw_df['text'] = raw_df['text'].replace('[^a-zA-Z0-9 ]', ' ', regex=True)\n","#Ensure that words are separated by single whitespace\n","raw_df['text'] = raw_df['text'].str.strip()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JrHRTcI6IchP"},"source":["#Split dataset between train and test sets\n","\n","train_size = 0.7\n","val_test_split = 0.5\n","\n","#Prepare train and test\n","train_df = raw_df.sample(frac=train_size,random_state=200)\n","test_val_df = raw_df.drop(train_df.index).reset_index(drop=True)\n","train_df = train_df.reset_index(drop=True)\n","\n","#Split test into two to have val set\n","test_df = test_val_df.sample(frac=val_test_split,random_state=200)\n","val_df = test_val_df.drop(test_df.index).reset_index(drop=True)\n","test_df = test_df.reset_index(drop=True)\n","\n","print(\"Dataset Shape: {}\".format(raw_df.shape))\n","print(\"Training Set Shape: {}\".format(train_df.shape))\n","print(\"Validation Set Shape: {}\".format(val_df.shape))\n","print(\"Test Set Shape: {}\".format(test_df.shape))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y_vun6poItp1"},"source":["#Preview train dataset distribution\n","EMOTION_LABELS = raw_df.columns.tolist()[2:]\n","train_df[EMOTION_LABELS].sum().sort_values().plot(kind=\"barh\");"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jEjLTrXBI5UX"},"source":["#Undersample neutral text. Randomly select 1000 instances and drop the rest from\n","#the dataframe\n","neutral_df = train_df[train_df.neutral == 1]\n","neutral_keep_df = neutral_df.sample(5000)\n","train_df = pd.concat([train_df, neutral_df, neutral_df]).drop_duplicates(keep=False)\n","train_df = pd.concat([train_df, neutral_keep_df])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0UIWH9eLJJsm"},"source":["## Tokenization"]},{"cell_type":"code","metadata":{"id":"cnzeXhk7JLSL"},"source":["#Use Cased version of BERT Base pretrained model\n","BERT_MODEL_FORMAT = 'bert-base-cased'\n","tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_FORMAT)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pvji1GKsJmPJ"},"source":["#Set the max token count to 512 to allow our model to process sentences with up\n","#to 512 tokens\n","MAX_TOKEN_COUNT = 512"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_h-5nMYaJwCE"},"source":["#Wrap the tokenization process in a Dataset and convert labels to tensors\n","#This code was taken from the reference mentioned above with only few\n","#tweaks made to it\n","\n","class TweetDataset(Dataset):\n","\n","  def __init__(\n","    self, \n","    data: pd.DataFrame, \n","    tokenizer: BertTokenizer, \n","    max_token_len: int = 512\n","  ):\n","    self.tokenizer = tokenizer\n","    self.data = data\n","    self.max_token_len = max_token_len\n","    \n","  def __len__(self):\n","    return len(self.data)\n","\n","  def __getitem__(self, index: int):\n","    data_row = self.data.iloc[index]\n","\n","    Text = data_row.text\n","    labels = data_row[LABEL_COLUMNS]\n","\n","    encoding = self.tokenizer.encode_plus(\n","      Text,\n","      add_special_tokens=True,\n","      max_length=self.max_token_len,\n","      return_token_type_ids=False,\n","      padding=\"max_length\",\n","      truncation=True,\n","      return_attention_mask=True,\n","      return_tensors='pt',\n","    )\n","\n","    return dict(\n","      Text=Text,\n","      input_ids=encoding[\"input_ids\"].flatten(),\n","      attention_mask=encoding[\"attention_mask\"].flatten(),\n","      labels=torch.FloatTensor(labels)\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fRR_DloVKF81"},"source":["#Tokenize training set\n","train_dataset = TweetDataset(\n","  train_df,\n","  tokenizer,\n","  max_token_len=MAX_TOKEN_COUNT\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_ccAp7ssKSLD"},"source":["#Wrap the dataset with the tokenization process into a LightningDataModule\n","#This code was taken from the reference mentioned above with only few\n","#tweaks made to it\n","\n","class TweetDataModule(pl.LightningDataModule):\n","\n","  def __init__(self, train_df, test_df, tokenizer, batch_size=8, max_token_len=128):\n","    super().__init__()\n","    self.batch_size = batch_size\n","    self.train_df = train_df\n","    self.test_df = test_df\n","    self.tokenizer = tokenizer\n","    self.max_token_len = max_token_len\n","\n","  def setup(self, stage=None):\n","    self.train_dataset = TweetDataset(\n","      self.train_df,\n","      self.tokenizer,\n","      self.max_token_len\n","    )\n","\n","    self.test_dataset = TweetDataset(\n","      self.test_df,\n","      self.tokenizer,\n","      self.max_token_len\n","    )\n","\n","  def train_dataloader(self):\n","    return DataLoader(\n","      self.train_dataset,\n","      batch_size=self.batch_size,\n","      shuffle=True,\n","      num_workers=4\n","    )\n","\n","  def val_dataloader(self):\n","    return DataLoader(\n","      self.test_dataset,\n","      batch_size=self.batch_size,\n","      num_workers=4\n","    )\n","\n","  def test_dataloader(self):\n","    return DataLoader(\n","      self.test_dataset,\n","      batch_size=self.batch_size,\n","      num_workers=4\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Eo8Zss0KoP7"},"source":["#Set our hyperparameters\n","N_EPOCHS = 3\n","BATCH_SIZE = 12\n","\n","data_module = TweetDataModule(\n","  train_df,\n","  val_df,\n","  tokenizer,\n","  batch_size=BATCH_SIZE,\n","  max_token_len=MAX_TOKEN_COUNT\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d11AivfwKwXL"},"source":["## Model Building"]},{"cell_type":"markdown","metadata":{"id":"54WbImvnK3cG"},"source":["*As described in our report, we use a pre-trained BERT module, specifically BERTBase uncased to build our model on top of. To give it multi-label text classification capacities, we add one linear layer as the classifier and a sigmoid activation function. We wrap it all up in a Lightning Module*"]},{"cell_type":"code","metadata":{"id":"pCcTbRhCKyKz"},"source":["#This code was taken from the reference mentioned above with only few\n","#tweaks made to it to adapt it to our multi-label text classification task\n","\n","class TweetTagger(pl.LightningModule):\n","\n","  def __init__(self, n_classes: int, n_training_steps=None, n_warmup_steps=None):\n","    super().__init__()\n","    self.bert = BertModel.from_pretrained(BERT_MODEL_FORMAT, return_dict=True)\n","    self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)\n","    self.n_training_steps = n_training_steps\n","    self.n_warmup_steps = n_warmup_steps\n","    self.criterion = nn.BCELoss()\n","\n","  def forward(self, input_ids, attention_mask, labels=None):\n","    output = self.bert(input_ids, attention_mask=attention_mask)\n","    output = self.classifier(output.pooler_output)\n","    output = torch.sigmoid(output)    \n","    loss = 0\n","    if labels is not None:\n","        loss = self.criterion(output, labels)\n","    return loss, output\n","\n","  def training_step(self, batch, batch_idx):\n","    input_ids = batch[\"input_ids\"]\n","    attention_mask = batch[\"attention_mask\"]\n","    labels = batch[\"labels\"]\n","    loss, outputs = self(input_ids, attention_mask, labels)\n","    self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n","    return {\"loss\": loss, \"predictions\": outputs, \"labels\": labels}\n","\n","  def validation_step(self, batch, batch_idx):\n","    input_ids = batch[\"input_ids\"]\n","    attention_mask = batch[\"attention_mask\"]\n","    labels = batch[\"labels\"]\n","    loss, outputs = self(input_ids, attention_mask, labels)\n","    self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n","    return loss\n","\n","  def test_step(self, batch, batch_idx):\n","    input_ids = batch[\"input_ids\"]\n","    attention_mask = batch[\"attention_mask\"]\n","    labels = batch[\"labels\"]\n","    loss, outputs = self(input_ids, attention_mask, labels)\n","    self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n","    return loss\n","\n","  def training_epoch_end(self, outputs):\n","    \n","    labels = []\n","    predictions = []\n","    for output in outputs:\n","      for out_labels in output[\"labels\"].detach().cpu():\n","        labels.append(out_labels)\n","      for out_predictions in output[\"predictions\"].detach().cpu():\n","        predictions.append(out_predictions)\n","\n","    labels = torch.stack(labels).int()\n","    predictions = torch.stack(predictions)\n","\n","    for i, name in enumerate(LABEL_COLUMNS):\n","      class_roc_auc = auroc(predictions[:, i], labels[:, i])\n","      self.logger.experiment.add_scalar(f\"{name}_roc_auc/Train\", class_roc_auc, self.current_epoch)\n","\n","\n","  def configure_optimizers(self):\n","\n","    optimizer = AdamW(self.parameters(), lr=2e-5)\n","\n","    scheduler = get_linear_schedule_with_warmup(\n","      optimizer,\n","      num_warmup_steps=self.n_warmup_steps,\n","      num_training_steps=self.n_training_steps\n","    )\n","\n","    return dict(\n","      optimizer=optimizer,\n","      lr_scheduler=dict(\n","        scheduler=scheduler,\n","        interval='step'\n","      )\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8zngAKOjMEI4"},"source":["#We define training steps based on our hyperparameters\n","steps_per_epoch=len(train_df) // BATCH_SIZE\n","total_training_steps = steps_per_epoch * N_EPOCHS\n","# We use part of our training steps to warm up\n","warmup_steps = total_training_steps // 4"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fePvj8pBMa3x"},"source":["#Model creation\n","model = TweetTagger(\n","  n_classes=len(LABEL_COLUMNS),\n","  n_warmup_steps=warmup_steps,\n","  n_training_steps=total_training_steps \n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lV2aiiqSMhgs"},"source":["## Model Training"]},{"cell_type":"code","metadata":{"id":"W13rkpYFMjcV"},"source":["#Remove saved lighting logs and checkpoints if any\n","!rm -rf lightning_logs/\n","!rm -rf checkpoints/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bSttYveqP-H3"},"source":["#Save top model checkpoint for recovery\n","checkpoint_callback = ModelCheckpoint(\n","  dirpath=\"checkpoints\",\n","  filename=\"best-checkpoint\",\n","  save_top_k=1,\n","  verbose=True,\n","  monitor=\"val_loss\",\n","  mode=\"min\"\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a5sdox-qQA_r"},"source":["#Use Tensor Hoard to log model's training\n","logger = TensorBoardLogger(\"lightning_logs\", name=\"Text\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fiXI7fo3YyZu"},"source":["#If validation loss hasn't improved in 2 epochs, stop epochs\n","early_stopping_callback = EarlyStopping(monitor='val_loss', patience=3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yD3UcgBHY-jZ"},"source":["#Define trainer with checkpoint and callback parameters\n","trainer = pl.Trainer(\n","  checkpoint_callback=checkpoint_callback,\n","  callbacks=[early_stopping_callback],\n","  max_epochs=N_EPOCHS,\n","  gpus=1,\n","  progress_bar_refresh_rate=30\n",")\n","\n","#Train model\n","trainer.fit(model, data_module)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uQGz4VbnZbXp"},"source":["#Load best model from trainer\n","trained_model = TweetTagger.load_from_checkpoint(\n","  trainer.checkpoint_callback.best_model_path,\n","  n_classes=len(LABEL_COLUMNS)\n",")\n","\n","trained_model.eval()\n","trained_model.freeze()\n","\n","#Save best model from trainer\n","torch.save(trained_model, 'goemotions_model.pth')\n","files.download('goemotions_model.pth')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5cIzfnFBZV_U"},"source":["## Model Evaluation"]},{"cell_type":"code","metadata":{"id":"bgYRXkAfZXBG"},"source":["trained_model = trained_model.to(device)\n","\n","val_dataset = TweetDataset(\n","  test_df,\n","  tokenizer,\n","  max_token_len=MAX_TOKEN_COUNT\n",")\n","\n","predictions = []\n","labels = []\n","\n","for item in tqdm(val_dataset):\n","  _, prediction = trained_model(\n","    item[\"input_ids\"].unsqueeze(dim=0).to(device), \n","    item[\"attention_mask\"].unsqueeze(dim=0).to(device)\n","  )\n","  predictions.append(prediction.flatten())\n","  labels.append(item[\"labels\"].int())\n","\n","predictions = torch.stack(predictions).detach().cpu()\n","labels = torch.stack(labels).detach().cpu()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tOYBnSeBatuK"},"source":["#Check model accuracy. We use a treshold of 0.5 to consider the presence\n","#or absence of an emotion\n","THRESHOLD = 0.2\n","accuracy(predictions, labels, threshold=THRESHOLD)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rr_FejN5a3Qf"},"source":["#Get AUROC for each emotion\n","print(\"Area Under The ROC Curve per Emotion\")\n","for i, name in enumerate(EMOTION_LABELS):\n","  tag_auroc = auroc(predictions[:, i], labels[:, i], pos_label=1)\n","  print(f\"{name}: {tag_auroc}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NAFmJqkNbWta"},"source":["#Get classification report for each emotion\n","y_pred = predictions.numpy()\n","y_true = labels.numpy()\n","\n","upper, lower = 1, 0\n","\n","y_pred = np.where(y_pred > THRESHOLD, upper, lower)\n","\n","print(classification_report(\n","  y_true, \n","  y_pred, \n","  target_names=LABEL_COLUMNS, \n","  zero_division=0\n","))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SDoBWYoXbkP1"},"source":["## Predictions"]},{"cell_type":"code","metadata":{"id":"teFCXNC8blZq"},"source":["def review_encoding(sent_model, review):\n","\n","  THRESHOLD = 0.2\n","  sentiments = []\n","\n","  encoding = tokenizer.encode_plus(\n","    review,\n","    add_special_tokens=True,\n","    truncation=True,\n","    max_length=512,\n","    return_token_type_ids=False,\n","    padding=\"max_length\",\n","    return_attention_mask=True,\n","    return_tensors='pt',\n","  )\n","\n","  _, sent_prediction = sent_model(encoding[\"input_ids\"], encoding[\"attention_mask\"])\n","  sent_prediction = sent_prediction.flatten().numpy()\n","\n","  for label, prediction in zip(LABEL_COLUMNS, sent_prediction):\n","    \n","    if prediction < THRESHOLD:\n","      continue\n","\n","    sentiments.append({\n","      \"sentiment\": label,\n","      \"prediction\": prediction\n","    })\n","\n","  return sentiments"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LD22DV2Tbq0q"},"source":["#Reload model if necessary\n","#reconstructed_model = torch.load(\"saved_models/goemotions_model.pth\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0YwG4B8Gb2ia"},"source":["#Get reviews saved from Script A\n","reviews = pd.read_csv('/datasets/reviews.csv')\n","#Keep relvant columns and clean up\n","columns = ['headline', 'pros', 'cons']\n","reviews['review'] = reviews[columns].apply(lambda row: '. '.join(row.values.astype(str)), axis=1)\n","\n","#Split review in sentences in order to get all emotions present\n","s = reviews['review'].str.split('.').apply(Series, 1).stack()\n","s.index = s.index.droplevel(-1)\n","s.name = 'review'\n","del reviews['review']\n","reviews = reviews.join(s)\n","\n","#Drop empty lines\n","reviews['review'] = reviews['review'].apply(lambda x: x.strip())\n","drop_empty = reviews[reviews['review'] == '']\n","reviews = pd.concat([reviews, drop_empty, drop_empty]).drop_duplicates(keep=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UuQxEyOrcphh"},"source":["#Run model on tokenized reviews\n","reviews['sentiments'] = reviews['review'].apply(lambda x: review_encoding(reconstructed_model, str(x)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XH5ig2T4cveg"},"source":["#Save review emotions for use in upcoming scripts\n","reviews.to_csv('review_sentiments.csv') \n","files.download('review_sentiments.csv')"],"execution_count":null,"outputs":[]}]}