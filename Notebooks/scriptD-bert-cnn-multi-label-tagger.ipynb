{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"final-bert-cnn-multi-label-tagger.ipynb","provenance":[],"authorship_tag":"ABX9TyO5hWNRPiYu4X7dJ1bEmPVi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"EbHNPVpfEMHp"},"source":["# Multi-label Text Classification Model based on BERT with CNN Classification layer. Trained on Posting Emotions Dataset [Script D]"]},{"cell_type":"markdown","metadata":{"id":"GoBb4TJLEpQP"},"source":["*This notebook contains the script used to build our main multi-label classification model, which recognizes emotions from job postings. In it, we build a BERTBase model with a CNN classification layer. Note that this notebook's code was written following a tutorial on multi-label text classification for tagging questions posted on Q&A sites such as Stack Overfllo. However, the content of the code was written to serve our own model goals.*\n","\n","---\n","*References: https://github.com/Moradnejad/Bert-Based-Tag-Recommendation*\n"]},{"cell_type":"markdown","metadata":{"id":"R7VYZVFeGB5M"},"source":["## Package Installation, Imports & Setup"]},{"cell_type":"code","metadata":{"id":"zuvLp7riEHSX"},"source":["pip install bert-tensorflow --quiet"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m2nMILARGN_8"},"source":["pip install bert-for-tf2 --quiet"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IqB1V8YCGP3n"},"source":["pip install sentencepiece"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v3qk7HcAGScv"},"source":["pip install tensorflow --quiet"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QXIctKKCGVIq"},"source":["pip install tensorflow_hub --quiet"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X5cqnFkzGW_L"},"source":["try:\n","    %tensorflow_version 2.x\n","except Exception:\n","    pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WTPSAZUKGbf1"},"source":["import bert\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","from tensorflow.keras import layers\n","\n","import pandas as pd\n","import numpy as np\n","import gc\n","import collections\n","import random\n","import re\n","import nltk\n","\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_recall_fscore_support\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","\n","from keras.callbacks import ModelCheckpoint\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","\n","\n","from bs4 import BeautifulSoup as bs\n","from collections import Counter\n","from nltk.stem import PorterStemmer, WordNetLemmatizer\n","nltk.download(\"popular\")\n","nltk.download(\"stopwords\")\n","from nltk import word_tokenize\n","from nltk.corpus import stopwords\n","cachedStopWords = stopwords.words(\"english\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cEZp4ZcPHADW"},"source":["## Data Cleaning Methods"]},{"cell_type":"markdown","metadata":{"id":"ivS5Hn4lHBtx"},"source":["The code cleaning techniques in this section was directly taken from the code in the reference mentioned in the title "]},{"cell_type":"code","metadata":{"id":"MvD2dJ_5HBHt"},"source":["def transliterate(line):\n","    cedilla2latin = [[u'Á', u'A'], [u'á', u'a'], [u'Č', u'C'], [u'č', u'c'], [u'Š', u'S'], [u'š', u's']]\n","    tr = dict([(a[0], a[1]) for (a) in cedilla2latin])\n","    new_line = \"\"\n","    for letter in line:\n","        if letter in tr:\n","            new_line += tr[letter]\n","        else:\n","            new_line += letter\n","    return new_line"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"29d2fY_fHgf4"},"source":["def text_cleaner(text,\n","                 deep_clean=True,\n","                 stem= True,\n","                 stop_words=True,\n","                 translite_rate=True):\n","    rules = [\n","        {r'>\\s+': u'>'},  # remove spaces after a tag opens or closes\n","        {r'\\s+': u' '},  # replace consecutive spaces\n","        {r'\\s*<br\\s*/?>\\s*': u'\\n'},  # newline after a <br>\n","        {r'</(div)\\s*>\\s*': u'\\n'},  # newline after </p> and </div> and <h1/>...\n","        {r'</(p|h\\d)\\s*>\\s*': u'\\n\\n'},  # newline after </p> and </div> and <h1/>...\n","        {r'<head>.*<\\s*(/head|body)[^>]*>': u''},  # remove <head> to </head>\n","        {r'<a\\s+href=\"([^\"]+)\"[^>]*>.*</a>': r'\\1'},  # show links instead of texts\n","        {r'[ \\t]*<[^<]*?/?>': u''},  # remove remaining tags\n","        {r'^\\s+': u''}  # remove spaces at the beginning\n","\n","    ]\n","\n","    if deep_clean:\n","        text = text.replace(\".\", \"\")\n","        text = text.replace(\"[\", \" \")\n","        text = text.replace(\",\", \" \")\n","        text = text.replace(\"]\", \" \")\n","        text = text.replace(\"(\", \" \")\n","        text = text.replace(\")\", \" \")\n","        text = text.replace(\"\\\"\", \"\")\n","        text = text.replace(\"-\", \" \")\n","        text = text.replace(\"=\", \" \")\n","        text = text.replace(\"?\", \" \")\n","        text = text.replace(\"!\", \" \")\n","\n","        for rule in rules:\n","            for (k, v) in rule.items():\n","                regex = re.compile(k)\n","                text = regex.sub(v, text)\n","            text = text.rstrip()\n","            text = text.strip()\n","        text = text.replace('+', ' ').replace('.', ' ').replace(',', ' ').replace(':', ' ')\n","        text = re.sub(\"(^|\\W)\\d+($|\\W)\", \" \", text)\n","        if translite_rate:\n","            text = transliterate(text)\n","        if stem:\n","            text = PorterStemmer().stem(text)\n","        text = WordNetLemmatizer().lemmatize(text)\n","        if stop_words:\n","            stop_words = set(stopwords.words('english'))\n","            word_tokens = word_tokenize(text)\n","            text = [w for w in word_tokens if not w in stop_words]\n","            text = ' '.join(str(e) for e in text)\n","    else:\n","        for rule in rules:\n","            for (k, v) in rule.items():\n","                regex = re.compile(k)\n","                text = regex.sub(v, text)\n","            text = text.rstrip()\n","            text = text.strip()\n","    return text.lower()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OjS4CTkQIijf"},"source":["# Convert html to regular text\n","def convert_html_to_text(data):\n","    soup = bs(data,'html.parser')\n","    body = soup.get_text()\n","    return body"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q0H_7ZqMJYFp"},"source":["## Data Import & Cleaning"]},{"cell_type":"code","metadata":{"id":"LN8dsyLyI1GN"},"source":["# Import top sentiments dataset\n","top_sentiments = pd.read_csv(\"top_sentiments.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9K3AJ1FMJE4q"},"source":["# Clean up text data\n","top_sentiments['listing'] = top_sentiments['listing'].astype('string')\n","top_sentiments['tags'] = top_sentiments['tags'].apply(lambda x: str(x).replace(\"nan\", \"\"))\n","top_sentiments[\"tags\"] = top_sentiments[\"tags\"].apply(eval)\n","top_sentiments['tags'] = top_sentiments['tags'].apply(lambda x: ' '.join(x))\n","top_sentiments.dropna(inplace=True)\n","print(top_sentiments['tags'])\n","top_sentiments['Text'] = top_sentiments['listing'].apply(text_cleaner)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PE98Zk4WJgDr"},"source":["# Define labels\n","\n","X = top_sentiments['Text'].tolist()\n","\n","#prepare tags\n","tag_list = []\n","for item in top_sentiments['tags']:\n","    temp = item.split(\" \")\n","    for word in temp:\n","        tag_list.append(word)\n","\n","\n","\n","tags = list(set(tag_list))\n","\n","\n","y = []\n","S=0\n","for item in top_sentiments['tags']:\n","    self_tags = []\n","    itemsplitted = item.split(\" \")\n","    \n","    for word in tags:\n","        if word in itemsplitted:\n","            self_tags.append(1)\n","        else:\n","            self_tags.append(0)\n","    \n","    values = np.array(self_tags)\n","    \n","    Y=all(values == 0)\n","    if Y==True:\n","        \n","        del X[S]\n","        S=S-1\n","    else:\n","        y.append(np.array(self_tags))\n","    S=S+1    \n","    \n","\n","y_list = []\n","for elem in y:\n","    y_list.append(elem.tolist())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p9QpAqR4HhRL"},"source":["## Tokenization"]},{"cell_type":"code","metadata":{"id":"CYU92H0WHic9"},"source":["#Import BERT Tokenizer and BERT Uncased from the TensorFlow Hub\n","BertTokenizer = bert.bert_tokenization.FullTokenizer\n","\n","bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n","                            trainable=False)\n","\n","vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n","\n","to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n","\n","tokenizer = BertTokenizer(vocabulary_file, to_lower_case)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sxVrnoHNK5Lr"},"source":["def tokenize_data(text):\n","    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EauOJHS0LAz0"},"source":["tokenized_data = [tokenize_data(text) for text in X]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WhizGcvILTQI"},"source":["# Split dataframe into trainand test set\n","tokenized_data_train,tokenized_data_test,y_list_train, y_list_test = train_test_split(tokenized_data, y_list, test_size = .2,random_state = 42)\n","print(\"Length of training data :\", len(tokenized_data_train))\n","print(\"Length of test data :\", len(tokenized_data_test))\n","\n","for c,item in enumerate(y_list_test):\n","    y_list_test[c] = np.array(item)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bxy5ji2JLlGb"},"source":["def column(matrix, i):\n","    return [row[i] for row in matrix]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xDqMr2FCLwBp"},"source":["## Model Creation"]},{"cell_type":"code","metadata":{"id":"PLW2bim_Lyky"},"source":["#bert text model\n","class TEXT_MODEL(tf.keras.Model):\n","    \n","    def __init__(self,\n","                 vocabulary_size,\n","                 embedding_dimensions=128,\n","                 cnn_filters=50,\n","                 dnn_units=512,\n","                 model_output_classes=2,\n","                 dropout_rate=0.1,\n","                 training=False,\n","                 name=\"text_model\"):\n","        super(TEXT_MODEL, self).__init__(name=name)\n","        \n","        self.embedding = layers.Embedding(vocabulary_size,\n","                                          embedding_dimensions)\n","        self.cnn_layer1 = layers.Conv1D(filters=cnn_filters,\n","                                        kernel_size=2,\n","                                        padding=\"valid\",\n","                                        activation=\"relu\")\n","        self.cnn_layer2 = layers.Conv1D(filters=cnn_filters,\n","                                        kernel_size=3,\n","                                        padding=\"valid\",\n","                                        activation=\"relu\")\n","        self.cnn_layer3 = layers.Conv1D(filters=cnn_filters,\n","                                        kernel_size=4,\n","                                        padding=\"valid\",\n","                                        activation=\"relu\")\n","\n","                                    \n","        self.pool = layers.GlobalMaxPool1D()\n","        \n","        self.dense_1 = layers.Dense(units=dnn_units, activation=\"relu\")\n","        self.dropout = layers.Dropout(rate=dropout_rate)\n","        if model_output_classes == 2:\n","            self.last_dense = layers.Dense(units=1,\n","                                           activation=\"sigmoid\")\n","        else:\n","            self.last_dense = layers.Dense(units=model_output_classes,\n","                                           activation=\"softmax\")\n","    \n","    def call(self, inputs, training):\n","        l = self.embedding(inputs)\n","        l_1 = self.cnn_layer1(l) \n","        l_1 = self.pool(l_1) \n","        l_2 = self.cnn_layer2(l) \n","        l_2 = self.pool(l_2)\n","        l_3 = self.cnn_layer3(l)\n","        l_3 = self.pool(l_3) \n","\n","\n","        \n","        concatenated = tf.concat([l_1, l_2, l_3], axis=-1)\n","        concatenated = self.dense_1(concatenated)\n","        concatenated = self.dropout(concatenated, training)\n","        model_output = self.last_dense(concatenated)\n","        \n","        return model_output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2udT0FDgLoBv"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"c7s1mhBgLo54"},"source":["# Method to log training progress\n","def progress(count, total, status=''):\n","    bar_len = 60\n","    filled_len = int(round(bar_len * count / float(total)))\n","\n","    percents = round(100.0 * count / float(total), 1)\n","    bar = '=' * filled_len + '-' * (bar_len - filled_len)\n","\n","    sys.stdout.write('[%s] %s%s ...%s\\r' % (bar, percents, '%', status))\n","    sys.stdout.flush()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WdVeAc7OL64A"},"source":["import sys\n","import os\n","import nltk\n","from nltk.corpus import reuters\n","from sklearn.preprocessing import MultiLabelBinarizer\n","import numpy as np\n","import random\n","import math\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import precision_recall_fscore_support\n","from sklearn.metrics import hamming_loss\n","import statistics \n","\n","\n","whole_predictions = []\n","whole_real_predictions = []\n","whole_threshold_predictions = []\n","text_model = []\n","\n","one=0\n","\n","#predict for each label individualy\n","\n","for i in range(len(y_list_train[0])):\n","\n","    whole_predictions = []\n","    whole_real_predictions = []\n","    whole_threshold_predictions = []\n","\n","    print(\"\\n\" + str(i)+\"\\'th label prediction started\")\n","    count_zero=0\n","    count_one=0\n","    new_label=[]\n","    new_tokenized_data_train=[]\n","    label = column(y_list_train,i)\n","    count_one=sum(label)\n","    print(\"count_one\",count_one)\n","    \n","    for k in range(len(label)):\n","        if count_zero< count_one and label[k]==0:\n","            new_label.append(0)\n","            new_tokenized_data_train.append(tokenized_data_train[k])\n","            count_zero=count_zero+1\n","        if label[k]==1:\n","            new_label.append(1)\n","            new_tokenized_data_train.append(tokenized_data_train[k])\n","\n","            \n","    print(\"count_zero\",count_zero)        \n","    data_with_len = [[data,new_label[j],len(data)]\n","                     for j,data in enumerate(new_tokenized_data_train)]\n","\n","    data_with_len.sort(key=lambda x: x[2])\n","    sorted_data_labels = [(data_lab[0], data_lab[1]) for data_lab in data_with_len]\n","    processed_dataset = tf.data.Dataset.from_generator(lambda: sorted_data_labels, output_types=(tf.int32, tf.int32))\n","    BATCH_SIZE = 14\n","    batched_dataset = processed_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()))\n","    TOTAL_BATCHES = math.ceil(len(sorted_data_labels) / BATCH_SIZE)\n","    TEST_BATCHES = TOTAL_BATCHES // TOTAL_BATCHES\n","    batched_dataset.shuffle(TOTAL_BATCHES)\n","    test_data = batched_dataset.take(TEST_BATCHES)\n","    train_data = batched_dataset.skip(TEST_BATCHES)\n","    \n","   \n","    VOCAB_LENGTH = len(tokenizer.vocab)\n","    EMB_DIM = 260\n","    CNN_FILTERS = 50\n","    DNN_UNITS = 256\n","    OUTPUT_CLASSES = 2\n","\n","    DROPOUT_RATE = 0.2\n","\n","    NB_EPOCHS = 6\n","\n","    text_model.append(TEXT_MODEL(vocabulary_size=VOCAB_LENGTH,\n","                        embedding_dimensions=EMB_DIM,\n","                        cnn_filters=CNN_FILTERS,\n","                        dnn_units=DNN_UNITS,\n","                        model_output_classes=OUTPUT_CLASSES,\n","                        dropout_rate=DROPOUT_RATE))\n","\n","    if OUTPUT_CLASSES == 2:\n","        text_model[i].compile(loss=\"binary_crossentropy\",\n","                           optimizer=\"adam\",\n","                           metrics=[\"acc\"])\n","    else:\n","        text_model[i].compile(loss=\"sparse_categorical_crossentropy\",\n","                           optimizer=\"adam\",\n","                           metrics=[\"sparse_categorical_acc\"])\n","\n","    text_model[i].fit(train_data, epochs=NB_EPOCHS)\n","\n","    self_label_predictions = []\n","    self_threshold_predictions = []\n","    self_label_real_values = []\n","    print(\"Predicting \" + str(i) + \"th label...\")\n","    \n","    for e,item in enumerate(tokenized_data_test):\n","        if e%2==0:\n","            progress(e,len(tokenized_data_test))\n","        res = text_model[i].predict([item])\n","        self_label_real_values.append(res[0][0])\n","      \n","        if res[0][0] > 0.93:\n","            self_threshold_predictions.append(res[0][0])\n","        else :\n","            self_threshold_predictions.append(0.0)\n","\n","    whole_threshold_predictions.append(self_threshold_predictions)\n","    whole_real_predictions.append(self_label_real_values)\n","\n","    whole_threshold_predictions = list(map(list, zip(*whole_threshold_predictions)))\n","    whole_real_predictions = list(map(list, zip(*whole_real_predictions)))\n","\n","            \n","    K_list= [3,5,10]\n","    for U in range(len(K_list)):\n","        K_tag_y_list_test = []\n","        k = K_list[U]\n","        sigma_recalls = 0\n","        sigma_precisions = 0\n","        sigma_f1score = 0\n","\n","        K_tag_y_list_test = y_list_test\n","        for f in range(len(K_tag_y_list_test)):\n","            progress(f,len(K_tag_y_list_test))\n","            currentitem = np.array(whole_threshold_predictions[f])\n","\n","            top_k_indexes = (-currentitem).argsort()[:k]\n","            \n","            for C in top_k_indexes:\n","                if whole_threshold_predictions[f][C] == 0.0 :\n","                    top_k_indexes = top_k_indexes[top_k_indexes != C]\n","          \n","            intercep = 0\n","            for numb in top_k_indexes:\n","                if K_tag_y_list_test[f][numb] == 1 :\n","                    intercep += 1\n","            num_of_exists_tags = np.count_nonzero(K_tag_y_list_test[f] == 1)\n","\n","            if len(top_k_indexes) == 0 :\n","                self_recall_k=0\n","            elif len(top_k_indexes) >= num_of_exists_tags :\n","                self_recall_k = intercep / num_of_exists_tags\n","            elif len(top_k_indexes) < num_of_exists_tags :\n","                self_recall_k = intercep / len(top_k_indexes)\n","            if len(top_k_indexes)==0:\n","                self_precisions_k=0 \n","            else:    \n","                self_precisions_k = intercep / len(top_k_indexes)\n","            if self_precisions_k==0 and self_recall_k==0:\n","                self_f1_score_k=0\n","            else:    \n","                self_f1_score_k = 2 * ((self_precisions_k*self_recall_k)/(self_precisions_k+self_recall_k))\n","            sigma_recalls += self_recall_k\n","            sigma_precisions += self_precisions_k\n","            sigma_f1score += self_f1_score_k\n","\n","        \n","        recall_k = sigma_recalls / len(K_tag_y_list_test)\n","        precisions_k = sigma_precisions / len(K_tag_y_list_test)\n","        f1score_k = sigma_f1score / len(K_tag_y_list_test)\n","        print(\"\\n\")\n","        print(\"\\n\" + str(i)+\"\\'th label metrics\")\n","        print(\"Recall@\"+ str(K_list[U])+\" = \" + str(recall_k))\n","        print(\"Precision@\"+ str(K_list[U])+\" = \" + str(precisions_k))\n","        print(\"f1score@\"+ str(K_list[U])+\" = \" + str(f1score_k))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xnOD6wAmL-Gq"},"source":["## Save model"]},{"cell_type":"code","metadata":{"id":"L_PYYCUJMCNs"},"source":["# Save model weights for reloading\n","for i,model in enumerate(text_model):\n","    model_name = 'bert_tagger' + str(i)\n","    path = 'bert_tagger_weights/'+model_name+'/ckpt'\n","    print(path)\n","    model.save_weights(path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f3KkXCLjMRWQ"},"source":["## Model Reload (if necessary)"]},{"cell_type":"code","metadata":{"id":"2jjRKcSOMWeH"},"source":["for i,model in enumerate(text_model):\n","  model_name = 'bert_tagger' + str(i)\n","  path = '/content/drive/MyDrive/bert_tagger_weights/'+model_name+'/ckpt'\n","  print(path)\n","  model.load_weights(path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FOkEbMCMMotf"},"source":["## Anvil Backend"]},{"cell_type":"code","metadata":{"id":"Cp00NDgkMndA"},"source":["# Download ANVIL package\n","!pip install anvil-uplink --quiet"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JGFjsWaIMw15"},"source":["# Connect to ANVIL app\n","import anvil.server\n","anvil.server.connect(\"D2UKWJF75275SOMVEC36ILFX-OCJ22E76KFJVXWHW\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h4rBatfDM1KT"},"source":["# Method to get listing interpretation\n","@anvil.server.callable\n","def predict_emotions(job_desc, sensitivity):\n","\n","  print('Reached backend')\n","\n","  postings = job_desc.split(\"\\n\")\n","  for posting in postings:\n","    posting = text_cleaner(posting)\n","  job_desc = text_cleaner(job_desc)\n","  postings.append(job_desc)\n","\n","  for posting in postings:\n","    if posting.strip() == '':\n","      postings.remove(posting)\n","    if len(posting.split()) < 4:\n","      postings.remove(posting)\n","\n","  tokenized_postings = [tokenize_data(data) for data in postings]\n","  \n","  for i, item in enumerate(tokenized_postings):\n","    if len(item) < 4:\n","      tokenized_postings.remove(item)\n","      del postings[i]\n","\n","\n","  predictions = []\n","  \n","  for i in range(len(text_model)):\n","    label_predictions = []\n","    for item in tokenized_postings:\n","      res = text_model[i].predict([item])\n","      label_predictions.append(res[0][0])\n","    predictions.append(label_predictions)\n","  \n","  prediction_df = pd.DataFrame(\n","    {'text': postings,\n","     'approval': predictions[0],\n","     'disapproval': predictions[1],\n","     'disappointment': predictions[2],\n","     'annoyance': predictions[3],\n","     'gratitude': predictions[4],\n","     'curiosity': predictions[5],\n","     'amusement': predictions[6],\n","     'caring': predictions[7],\n","     'optimism': predictions[8],\n","     'realization': predictions[9],\n","     'excitement': predictions[10],\n","     'confusion': predictions[11],\n","     'joy': predictions[12],\n","     'anger': predictions[13],\n","     'fear': predictions[14],\n","     'nervousness': predictions[15],\n","     'sadness': predictions[16],\n","     'desire': predictions[17]\n","    })\n","  \n","  text_emotions = []\n","  for index, row in prediction_df.iterrows():\n","    emotions = []\n","    for i in range(1,len(row)):\n","      if row[i] > sensitivity:\n","        emotions.append(prediction_df.columns[i])\n","    text_emotions.append({'sentence_index':index, 'emotions':emotions, 'sentence': row[0]})\n","  \n","  return text_emotions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p6RDuvb_M8dx"},"source":["anvil.server.wait_forever()"],"execution_count":null,"outputs":[]}]}