{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"final_review_collection_script.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN8fsSBu5GsPCwUNA0qGoIp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"-jCHiCqFzOGC"},"source":["# Review Collection Script [Script A]"]},{"cell_type":"markdown","metadata":{"id":"GohDlKQezQ2n"},"source":["*This notebook contains the script used to collect reviews from the Glassdoor job posting site. We use the python library Selenium to carry out the scraping. Note that this notebook's code was heavily inspired by the script written by Maria Vasilenko for Glassdoor scraping, published on Medium. However, this inspiration only appears in the structure of the code. The content of the code was written from scratch to serve our own scraping goals.*\n","\n","---\n","*References: https://mashavasilenko.medium.com/scrape-your-way-to-thousands-of-interview-reviews-f6dba8063539*\n"]},{"cell_type":"markdown","metadata":{"id":"aeKrF8XF0d5k"},"source":["## Package installations and imports"]},{"cell_type":"code","metadata":{"id":"ftsboghazKmz"},"source":["!pip install selenium --quiet\n","!apt-get update --quiet # to update ubuntu to correctly run apt install\n","!apt install chromium-chromedriver --quiet\n","!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n","import sys\n","sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n","from selenium import webdriver\n","chrome_options = webdriver.ChromeOptions()\n","chrome_options.add_argument('--headless')\n","chrome_options.add_argument('--no-sandbox')\n","chrome_options.add_argument('--disable-dev-shm-usage')\n","driver = webdriver.Chrome('chromedriver',chrome_options=chrome_options)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DKbkCiZM0nlh"},"source":["from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException, TimeoutException, StaleElementReferenceException\n","from selenium import webdriver\n","from selenium.webdriver.common.by import By\n","from selenium.webdriver.support.ui import WebDriverWait\n","from selenium.webdriver.support import expected_conditions as EC\n","\n","import time\n","import pandas as pd\n","import re\n","import math"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PsaoQtDp0oXm"},"source":["VERBOSE = False #Used for debugging"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ch2b8OF-0uRw"},"source":["## Notebook Methods"]},{"cell_type":"markdown","metadata":{"id":"xFJFJDWO00Ur"},"source":["### Login method"]},{"cell_type":"markdown","metadata":{"id":"0Ivh_ef_4z-l"},"source":["*This method makes sure we are logged in and have full access to the Glassdoor website*"]},{"cell_type":"code","metadata":{"id":"FznTpITC0xEz"},"source":["## Login Method\n","## Code based on login method from https://mashavasilenko.medium.com/scrape-your-way-to-thousands-of-interview-reviews-f6dba8063539\n","\n","def login(driver, login_url, email, password):\n","    \n","    try:\n","        driver.get(login_url)\n","        if VERBOSE:\n","            print('Arrived at login page')\n","    except:\n","        print('Failed to find login page')\n","    \n","    # Navigate to the e-mail field\n","    try:\n","        if VERBOSE:\n","          print('Finding email field')\n","        email_field = driver.find_element_by_xpath(\"//*[@type='submit']//preceding::input[2]\")\n","    except NoSuchElementException:\n","        if VERBOSE:\n","            print('Could not find email field')\n","    # Send e-mail\n","    email_field.send_keys(email)\n","    \n","    # Navigate to the password field\n","    try:\n","      if VERBOSE:\n","          print('Finding password field')\n","      pwd_field = driver.find_element_by_xpath(\"//*[@type='submit']//preceding::input[1]\")\n","    except NoSuchElementException:\n","        if VERBOSE:\n","            print('Could not find password field')\n","    pwd_field.send_keys(password)\n","    pwd_field.submit()\n","    time.sleep(3)\n","    print('Successful login')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2-mXHBcW2RF-"},"source":["### Accepting Recommended Cookies Method"]},{"cell_type":"markdown","metadata":{"id":"60hj-mg7477H"},"source":["*This method ensures that we are not blocked by the accept cookie prompt that usually pops up in all sites, following the introduction of GDPR restrictions*"]},{"cell_type":"code","metadata":{"id":"YkcK7zTW2WLs"},"source":["def accept_cookies(driver):\n","    #Accepting recommended cookies\n","        try:\n","            accept_button = driver.find_element_by_id('onetrust-accept-btn-handler')\n","            accept_button.click()\n","            if VERBOSE:\n","                print('Recommended cookies accepted')\n","        except NoSuchElementException:\n","            if VERBOSE:\n","                print('No accept recommended cookies button')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"94G3bPQ44b8d"},"source":["### Get Company IDs Method"]},{"cell_type":"markdown","metadata":{"id":"rGbU9wCh4lix"},"source":["*This method allows us to collect a list of companies with technical positions within the United States, along with their IDs. The company names and IDs on Glassdoor will be needed to collect their reviews*"]},{"cell_type":"code","metadata":{"id":"V9oQ5yhx4kmV"},"source":["def get_company_ids(page_limit):\n","    \n","    #Accepting recommended cookies if necessary\n","    accept_cookies(driver)\n","        \n","    url = 'https://www.glassdoor.com/Explore/browse-companies.htm?overall_rating_low=0&page=1&isHiringSurge=0&locId=1&locType=N&locName=US&occ=Data'\n","    driver.get(url)\n","    \n","    \n","    #Give time for the page to load\n","    time.sleep(4)\n","    \n","    #Accepting recommended cookies\n","    accept_cookies(driver)\n","    \n","    \n","    #Initialize list of companies\n","    companies = []\n","    \n","    #Get the number of pages of companies which fit the search criteria. If too many pages, limit it to 100 pages\n","    companies_count = driver.find_element_by_xpath('//span[@class=\"common__commonStyles__subtleText resultCount\"]').find_elements_by_tag_name('strong')[2].text\n","    num_pages = math.ceil(int(companies_count)/10)\n","    if num_pages > page_limit:\n","        num_pages = page_limit\n","    current_page = 1\n","    \n","    #Loop through pages and get companies\n","    while current_page <= num_pages: \n","        \n","        #Give time for the page to load\n","        time.sleep(4)\n","    \n","        employer_cards = driver.find_elements_by_xpath('//section[@class=\"employerCard__EmployerCardStyles__employerCard common__commonStyles__module p-std mt-0 mb-std mx-std mx-sm-0\"]')\n","        for employer_card in employer_cards:\n","            link = employer_card.find_elements_by_tag_name('a')[0].get_attribute('href')\n","            relevant_info = re.search(\"Reviews/(.*)-Reviews-(.*)\\.\", link)\n","            if relevant_info:\n","                company_name = relevant_info.group(1)\n","                company_id = relevant_info.group(2)\n","\n","            companies.append({\"company_id\":company_id, \"company_name\":company_name})\n","        \n","        #Go to next page\n","        try:\n","            next_button = driver.find_element_by_css_selector(\"[aria-label=Next]\")\n","            next_button.click()\n","            current_page +=1\n","            print(\"Successfully moved to next page \"+str(current_page))\n","        except NoSuchElementException:\n","            print ('No more pages to scan')\n","            break\n","    \n","    return companies\n","        "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wa2brVFv5YY_"},"source":["### Get Company Reviews"]},{"cell_type":"markdown","metadata":{"id":"2o00qi-s5cfR"},"source":["*This method scrapes Glassdoor to get reviews from the companies that are passed in as a parameter. If at any point, the review collection fails, the method returns the reviews it has managed to collect*"]},{"cell_type":"code","metadata":{"id":"kjhmSy2v5XST"},"source":["def get_reviews(companies, page_limit, debug):\n","    \n","    \n","    #Accepting recommended cookies\n","    #accept_cookies(driver)\n","    \n","    \n","    column_names = ['company', 'rating', 'employee', 'headline', 'jobtitledate', 'pros', 'cons']\n","    reviews = pd.DataFrame(columns = column_names)\n","    \n","    try:\n","\n","      for company in companies:\n","          \n","          url = 'https://www.glassdoor.com/Reviews/'+company['company_name']+'-Reviews-' + company['company_id'] +'.htm?sort.sortType=RD&sort.ascending=false&filter.iso3Language=eng'\n","          driver.get(url)\n","      \n","          #Give time for the page to load\n","          time.sleep(4)\n","\n","\n","          #Accepting recommended cookies\n","          accept_cookies(driver)\n","\n","          #Setup lists of data to be extracted\n","          ratings, employee_types, headlines, job_title_dates, pros, cons = ([] for i in range(6))\n","\n","          #Get the number of pages of review. If too many pages, limit it to 100 pages\n","          pagination_element  = driver.find_element_by_class_name(\"paginationFooter\").text\n","          pagination = re.search('of (.+?) Reviews', pagination_element)\n","          if pagination:\n","              num_pages = int(pagination.group(1).replace(\",\", \"\"))\n","          if num_pages > page_limit:\n","              num_pages = page_limit\n","          current_page = 1\n","          \n","          try:\n","              #Loop through pages and get reviews\n","              while current_page <= num_pages: \n","\n","                  if VERBOSE:\n","                      print('Moving through reviews for company ' + str(company['company_name']))\n","\n","                  #Let the page load. Change this number based on your internet speed.\n","                  #Or, wait until the webpage is loaded, instead of hardcoding it.\n","                  time.sleep(4)\n","\n","                  #Going through each review on page\n","                  review_boxes = driver.find_elements_by_class_name(\"gdReview\")\n","                  rating_spans = driver.find_elements_by_xpath('//span[@class=\"ratingNumber mr-xsm\"]')\n","                  employee_type_spans = driver.find_elements_by_xpath('//span[@class=\"pt-xsm pt-md-0 css-1qxtz39 eg4psks0\"]')\n","                  headline_spans = driver.find_elements_by_class_name(\"reviewLink\")\n","                  job_title_date_spans = driver.find_elements_by_class_name(\"authorInfo\")  \n","                  \n","                  for rating_span in rating_spans:\n","                      ratings.append(rating_span.text)\n","\n","                  for employee_type_span in employee_type_spans:\n","                      employee_types.append(employee_type_span.text)\n","\n","                  for headline_span in headline_spans:\n","                      headlines.append(headline_span.text)\n","\n","                  for job_title_date_span in job_title_date_spans:\n","                      job_title_dates.append(job_title_date_span.text)\n","                      \n","                  ##TEST\n","                  for review_box in review_boxes:\n","                      pros_cons_div = review_box.find_elements_by_class_name(\"v2__EIReviewDetailsV2__fullWidth\")\n","                      if len(pros_cons_div) == 2:\n","                          if VERBOSE:\n","                              print('Both pros and cons found')\n","                          for div in pros_cons_div:\n","                              paragraphs = div.find_elements_by_tag_name('p')\n","                              if paragraphs[0].text == 'Pros':\n","                                  pros.append(paragraphs[1].text)\n","                              elif paragraphs[0].text == 'Cons':\n","                                  cons.append(paragraphs[1].text)\n","                      elif len(pros_cons_div) == 1:\n","                          if VERBOSE:\n","                              print('Only pros OR cons found')\n","                          paragraphs = pros_cons_div[0].find_elements_by_tag_name('p')\n","                          if paragraphs[0].text == 'Pros':\n","                              if VERBOSE:\n","                                  print('Only pros found')\n","                              pros.append(paragraphs[1].text)\n","                              cons.append('NA')\n","                          elif paragraphs[0].text == 'Cons':\n","                              if VERBOSE:\n","                                  print('Only cons found')\n","                              pros.append('NA')\n","                              cons.append(paragraphs[1].text)\n","                      else:\n","                          if VERBOSE:\n","                              print('No pros or cons')\n","                          pros.append('NA')\n","                          cons.append('NA')\n","\n","\n","                  #Go to next page\n","                  try:\n","                      driver.find_element_by_class_name(\"nextButton\").click()  #Find next page button and click\n","                      current_page +=1\n","                      if VERBOSE:\n","                          print(\"Successfully moved to next page \"+str(current_page))\n","                  except NoSuchElementException:\n","                      print ('No more pages to scan')\n","                      break\n","          except:\n","              print('Review collection failed for company '+ str(company['company_name']))\n","\n","          company_reviews = pd.DataFrame({'company': company['company_name'], 'rating': ratings, 'employee': employee_types, 'headline': headlines, 'jobtitledate': job_title_dates, 'pros': pros, 'cons' : cons})\n","          reviews = pd.concat([reviews, company_reviews])\n","    except:\n","      print('Review collection stopped')\n","    finally:\n","      return reviews\n","    return reviews"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sPhsb8mZ6CF7"},"source":["## Review Collection"]},{"cell_type":"code","metadata":{"id":"vtOFfiQr6Fqs"},"source":["#Login to Glassdoor\n","login(driver, 'https://www.glassdoor.com/profile/login_input.htm', 'seckml17@gmail.com', 'seckadjamberry02')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SUySUdiq6O8Z"},"source":["#Collect a list of companies with Data roles in the United States.\n","company_list = []\n","\n","#Note that this line will collect 1000 companies (10 companies per page for 100 pages).\n","#In reality, we broke down the collection per 200 companies and ran this script multiple \n","#times over a three-month period \n","company_list.append(get_company_ids(100))\n","\n","#We export the company list to CSV in case we need access to it in other scripts\n","company_list = [c for company in company_list for c in company]\n","import csv\n","keys = company_list[0].keys()\n","company_file = open(\"company_list.csv\", \"w\")\n","dict_writer = csv.DictWriter(company_file, keys)\n","dict_writer.writeheader()\n","dict_writer.writerows(company_list)\n","company_file.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sd6euSVx6-yW"},"source":["#Collect company reviews based on company list\n","#Note that this line will collect 1000 reviews per company (10 reviews per page for 100 pages).\n","#In reality, we broke down the collection and ran this script multiple times over a three-month \n","#period \n","reviews = get_reviews(company_list, 100, True)\n","#Clean up reviews collected and split the jobtitledata column into the data and title columns\n","reviews.reset_index(drop=True, inplace=True)\n","reviews['date'] = reviews['jobtitledate'].str.split('-',expand=True)[0]\n","reviews['title'] = reviews['jobtitledate'].str.split('-',expand=True)[1]\n","reviews.drop(columns='jobtitledate')\n","\n","# We export the company list to CSV in case we need access to it in other scripts\n","reviews.to_csv('review_data.csv') \n","files.download('review_data.csv')"],"execution_count":null,"outputs":[]}]}